## Meeting
Calendar Event Title: incident analysis (join)
Calendar Event Time: 2025-09-17T19:00:00-04:00
Meeting Note Title:incident analysis (join)
Attendees: 
Link: https://notes.granola.ai/d/db5215f8-ccf2-4290-9887-52b3b822b6f3

## Transcript
 
Me: Come about. Let's say you duplicate or you delete 150 thousand things. And then the whole system assumes you don't have 150 thousand journal entries. It does. A bunch of stuff. Based on that assumption. So then when the data comes back in. Do those things get reset or not? You might have told a user, hey, you made $10,000 when you actually made 20 again. Just make an example. Here. Or matching is probably more trickier. Algorithm. Any questions so far? Okay? So as a refresher. We get data from an accounting system like QuickBooks. The way we do it right now is QuickBooks sends their data to an API aggregated called Merge. And merge sensitive basis. This is what the old system looks like. So QDO has some data structure that looks like a QBO data structure. Let's say it's one of the world's worst data structures, but whatever. It has an ID, and this QuickBooks ID is like 1, 2, 3. And it has, like, a name or something for an object. And then merge has their own id, which is usually a use UID. And it says like some UUID. But they also have a remote ID, which is the ID of the underlying system in this case would be 1, 2, 3. And then they'll have another field called, then they'll have their own. In this case, let's say they call this word name here. They might call this word description. Here. But copy that same value. From year to year. And then to get the original data, they have a field called remote underscore data. Where they'll give you this exact object. They'll just stick it inside the object. That way, if merges fields don't cover something, You can get some underline data from inside the remote. Data? I will get the concept. Okay? In most accounting systems. When you go and create an expense and we've said this each week, but I always like to remind you some account system state. I would like to create an expense. I bought sandwich fil a, so that sandwich is $15. And you say chick fil a and maybe add a receipt to it. And you do a bunch of stuff. To it. The accounting software automatically creates a journal entry. Object. And the general empty will have a negative 15 but also a positive 51 for this year. Chase Credit Card. 1, 2, 3, 4. Positive 15 of meals. Classic journal entry. Anyone have a question about that? Assuming that based on. Zero. Then you could query the expenses and pointing out on the expenses. And you can query that journal entries table to get all the journal entries. And so they will have duplicative information. And then you also can query for these will have a link to the other one. So when, according to J table, it might have a little bit of the expense that generated it. Or vice versa or something, so that's good. Quickbooks doesn't work this way. Oh, and then one more thing. One of the building blocks of basis is journal entry line items. This. Negative 50. Negative, positive 15. These two lines. Is what all of our opinions, all of our the entire platform is built on top of journal entry line apps or dashboards or tasks or like 90% of the work that we do is building a concept of geometry line, which is not how software works. And I debated this for a while. And you land it on Chilean items are the truth. Of what? Is in your account. Ing. And everything else is just helper functions. What's the debate over? Like, what's the other. Well, it was like, you could do say that like they're all matter. Right. Expenses matter and deposits matter. They all equally matter. And we said no, the whole point of the entire assessment is to get journal in July items. Disagrees with journal entry line items. Journals is the correct one. Journal Adams is always the part. Let me rephrase. It's not the Chronicle. It's the thing that matters. It makes like most of accounting is built on top of your ledger data. And so getting that ledger data is the only thing that really matters. Getting a bunch of expenses doesn't really matter. You don't build a balance sheet or statement based on your expenses. You build or help with your geologist. You know, you don't do analysis on flipping expenses. You do it on top of the challenges. Usually. Everything's on top of the general options. These are all avenues through which you make journal entries. Easier. But everything's built on opportunities. Generally just have the most important thing. Question so far. Also, Ashley, is this Granola? Yes. Ashley's horse. Thank you, ashley. Okay. So in stage, in zero, you will get these two objects separate. In QuickBooks. It's very interesting. When you go and create an expense in QuickBooks. Increase in expense. That's great. It does not create a journal entry. In QuickBooks view, which we think is a very dumb view. The ledger, which is called the general ledger, right? This is the book and all the official information. The ledger, which is the truth of what's happening. If you want to see the ledger, it is generated at runtime like a CSV. Yes. I did not know that. Yes, See, that's where things get very trick. It is the wonkiest object I've ever seen. It is like a nested CSV object. It looks like the world has generated at one time. The jellies do not exist. That's crazy. Yeah. Is that where we. Have the ability to create manual journal entries directly. If they're not expense deposit transferring to this object, you just have, like, three lines. You want to write it down. But in QuickBooks, it's almost written as if it's this side. Where it's just like another object equivalent to expenses, not separate from it. Does that make sense? Now, when we first started the company and we wanted to build off jellies, we said, well, if I have an expense object, There's two options. Either. I can try to construct the journal entry that expense is creating ourselves. Looking at expense object and trying to figure out how to make a journal of this. We made the decision that that's super hard to do. We think that maybe there's a magic to doing that that we might not know, and we didn't have to, like, figure out all the magic for that. Instead, if you just hit their CSV generator, It would give you two lines. Like, negative 55, this expense. Except those lines themselves don't have, like, IDs or anything. Like, a CSV line. There's no, like, unique identifier for them. How does merge extract drive. So this is the other thing. Merges four QuickBooks specifically. Now Merch now has done something different after we gave them a bunch of feedback. We haven't used it. We don't trust it. Their journal entries object only queried the manual Journal entries entered into QuickBooks. It will not give you the journal entries that are derived from the expense objects. So it's not a full ledger. It's just a manual geometry or ones that they enter it direct. Ly now could merge as a new endpoint. Called generated journal entries. So they made a. So what? They're saying their pitch is. We did all the math. Or they're using this like report. To generate the equivalent journal entries for this expense object. But they release that, like, a few months ago. And I actually have no idea if that works or not. Just knowing how painful this was. Maybe it does work. Maybe it doesn't work. I don't want to find out. We don't want to build more stuff. On. So maybe it works now, but when we were starting for last year and a half, you would just get manual journal entries. And so we were left to the question of if you were in the building system, we're talking journal in July. It would have to be either from that CSV or calculating it ourselves. We opted for CSV. This is what has led to a lot of nightmares. Fun facts about CSVs. If this object gets deleted, these two lines disappear from the report. Because it's auto generated. How do we know in our system that it's been deleted? Very tricky information. They just kind of disappear. You know or find other fact. Let's say you change this from negative 15 to negative 17. Now, there's two lines here that say negative 16, positive 17. But in our system, it says negative 15 and positive 50. There's no unique identifiers. So how do I know which line to update? How do I know which one to deletion? I like your face. That's the correct. That's the correct response. So these are all various. So you'll see a function called Delete Ghost child objects. What? There are ghosts? And the ghost is. When you first started, it was negative 15. And now it's negative 17. 17. So what we do is we create both of these. And then we say, wait a second, I didn't see 15. Negative 15. And they're now ghosts. They didn't show up. It takes a live object as an infrastructure. I didn't write this function. I just want to notepad again. Okay. It says you modified it two weeks ago. No, I moved stuff around. I did not write the function. This function is elastic ghost children. Ids. So anyway. So there's. There's some ghost stuff going on there. Some of the stuff, it's just the byproduct. Of using a generated CSV file. To power an entire ledger system. Like 90, I don't know, maintenance number up, but like 80%. 90% of all your data's ended through expenses. Deposits transfer, credit card payments, vendor current is, credit models, all these other things, tactics, all this stuff. And so you never actually touched on the ledger. And so all of the QuickBooks data is coming. From the RCSV generated reports that we then have to then manage all the data lunging for. And then people use our product on top of that CSV report. Other things about the CSV report. It does not tell you something's changed. It doesn't tell you. It can't tell you, right? Like someone goes back 10 months. Someone goes and edits a journal entry from a year ago. Who's telling me that that happened? What did CSV the way we used to do it. I just rewrote this all in a much better way. I made this code. It has some bugstar, I know, but it's much better. But for the longest time it would just say, give me the last 90 days of data. In a continuous loop. But that meant that if data was changed outside the 90 day window for that CSV generated, then we wouldn't know. We just put it. It would just be on a date in a row. And that's why we don't use jellies for trial balances, because a lot of them are wrong. Oh, because we don't under. They're just wrong. Now, the new code actually fixes this and that. Fixes it on an ongoing basis. I mean, it doesn't fill the backfill, but that issue won't happen again. Why don't we talk about how that works? And so up until, like, three weeks ago, Four weeks ago. How the ETL would run is it would get manual journal entries from Merge. And process those. Okay? Then it would take the CSV report. Generate that. Go through all the lines. If it found that the merged journal entry existed, it would skip over that line. It would set that object to be unprocessable. Say there's a merge equivalent. Because both the journal report CSV and the merged data are both giving you the same data. Journal entries from Merge. This is also the other thing. The whole time? Yes. Anyways. Now, one of the reasons is because in retrospect, that was not maybe the best architectural decision. But at the time, I was thinking I was alone in the room, guys. Just have everything, ok? Feel sorry for me. In merge. Merge. Because these are manual journal entries, actually had ids for their lines, IDs for their objects. They had. All the properties you want is deleted, not is deleted. All these other things. So the management of the these are super simple. You know, you don't have to do the Ghost child stuff and all this other crazy stuff. Now, in retrospect, if you're gonna do it for 80% of them anyways, why would you just do it for the last 90%? You know? But the way I wrote it when I first joined the company is. I was like, this data is better. Use this. And then use this as a backup, which is also one of the reasons why we're having so many bugs. That decision is now eating me. As a magnito. So when the CSV gave you a duplicate now you have two journal entries of the exact same data. I will just take this version of it and I would mark it as this would feel and then grab just on unprocessable. And I would give it. There's a field called unprocessable reason, and it would say found the exact same thing in merge. Not gonna try to make a journal entry out of this. This is a duplicate. Is that? Unprocessable serve any other purpose for any other vendor. Algorithms. Yeah, a lot of reasons to be unprocessable. So, for example, sometimes. The CSV would generate a date of 00. That would be the official date. In the CSV report. And you go, wow, that's not a real date. And so you go unprocessable. That doesn't make any sense. And we have to go back to those and see if the data doesn't make any sense because we're reporting a mistake. The CSV generator would do all kinds of crazy stuff. Sometimes amounts would be null, accounts would be null, counterparties being all the time. But the things you would expect to be there feels really. This is the other reason why I use manual journaling. If you're ever possible. Because this generated CSV is absolute bullshit. We would find a lot of. Not a lot, but there'd be some errors with that. When you plan errors. Do you flag them? Because when we process, we don't have a flagging process. So what I would do, again, I was one person, guys. I would periodically look at all the unprocessables and I would group them by an issue type. And I would take majority issues, and I'd be like, what is going on here? And then I'd go fix that. Oh, fun fact about the report, okay? So let's say you have an account that's under current assets. So because current assets, which is an account category underneath that, you would have some chase. 1, 2, 3, 4. In the report. It could either. And this has a nominal code, let's say 1,010. Anyone know what an omn code is? It's like a unique identifier. So. Probably merge statistically. And so when you're downloading a CSV report, It was kind of interesting. You would either have a call the account chase. One, two, three, form. Or we would call it 10010. Chase. 1, 2, 3, 4. Or it could call it current. Assets. 1, 2, 3, form. Or it could call it 10010 current assets. Then call in Chase. 1, 2, 3, 4. And if the account was deleted. The string would change to space. Deleted. As they count. Name? I've seen that somewhere. Yes. They had no standardization of how they would show you account names. And so there's a function right now in the code, so I'm going to try all of them. So that's, like, another thing you have to do. And then sometimes you miss an account. Right now I can't find an account because it's called, like, I don't even know. I don't even know what this account is called. It's actually. And I'm letting it go. Now. Yeah. Any questions so far? Now, one of the mistakes items with this code. In a while this code is written again. Year and a half ago. I was using strings to match account names for the journal entry line items. Turns out there were IDs. So this is one of the bugs I'm dealing with right now, which is that two years ago, I made a decision to use string matching. Don't know why. But there turned out to be ids, which seem there was some. Actually, there was a reason for this. Oh, a lot of times the IDs were missing. This would also be this. It's a CSV generated report. And they have bugs. I don't know. Maybe they fix it now. But super unreliable. But the string was always there, the ID for sometimes out there. It's unsec. It's a dictionary object, but. Like, if you think about those, like, balance sheet reports and stuff, there's, like, tables. And different sections. They essentially take, like, a complicated Excel file and try to make it a JSON. With, like, weird nesting rules, and if you look at the object, it's actually infuriating. She's code interpreter. She's. If you look at that code. I don't know if I've been code interpreter, maybe now can figure it out, but there were so many cases. We got wrong. That's a good point. Screw all of our deterministic code. That's actually what the bashing algorithm is. It's just telling the model to do everything. Okay? Any questions on this CSV object? Okay? So now we're getting off of merge. And that's also its own fun nightmare. I can describe this problem in a few steps. So first of all, we're saying. This journal report is now officially true, even for the manual journal entries. Because that's how you get off of merge, right? You get off a merge, you have to now switch to using the CSV for all the geometries. So when I wrote that code, what would happen is the CSV will generate manual geometries and then the code would find equivalent merged journalist that already exists. Right. And then it would have to swap it out. Remember that every journal entry is a client object. And every client object is a vendor object. Now, this journal entry. Things have been built on top of this, like matching, you know, transactions, product, whatever. Tasks. You know, recs have been built on top of this journal entry. If I create a new one and delete this one. All of these things will now break. So what we have now is a new vendor Object from the QuickBooks Journal report. And I just swapped this one out. For that one. That's how the code pattern works. And then you take the old vendor object from Merge and you set that to unprocessable. Because now that's unprecedented. This is easy to do on journal entries. Much harder to do on journal entry line items. So I'll give you an example. Merge have a journal entry. Let's say. Let's say there's some journal entry 1, 2, 3, 4. This is the id one. Merge would have a journal entry that would have the lines for an MMC negative 100, positive 100. Great. I turn off the merge etl. And I turn on the quickbooks new etl. And the quickbooks. You know, the CSP says 1, 2, 3, 4. That I know, but the individual lines don't have IDs. A QuickBooks one comes out and says, oh, it's actually negative 50. Negative or negative 25? Negative 75. Positive 100. Now, just like I said, for journal entries, I have to swap out the vendor objects whenever possible. I have to do the same with the journal entry line items themselves, which are also fine objects built on top of vendor objects. So whenever I can, if I match these, I would just swap the vendor object of that journal entry line item. But in this case, Where all of a sudden, the data looks different. I do have to then mark this as deleted and then create these two. Why do you have to go back and redo this? Can you just switch over to the new one? Replace these. Yeah. So each of the. So let's say there's a journal entry, and then it has two lines. So from merge, we have three objects. And there's things built on top of those three objects. So whenever possible, I'm trying to switch out the vendor object of those three objects. Because if I created new ones, then I have to swap all the things to point to that. Yeah, I found that. What I'm saying is, let's say starting from September. Or it's october 1st. Yeah. Make switch. Yes. We just use those only apply the new QBR just going forward so you don't don't touch the yeah, but people kept updating all ones people are going back to changing stuff in time. Yeah. Yeah. That's what broke my system. It's always the user's fault. Why is there a molecule discrepancy? Like muge got something wrong? Well, what happened is from the time I turn off the merge ETL to the time I got the data. Things had changed. And so when I didn't, because I didn't cast that initially, what would happen is in this case, it would leave all. They would match these, swap it out, and then it would keep all of these lines. And so the sum of the journal entries would be negative 100. As opposed to zero. Makes sense. And that would be wrong, because all general entries have to add up to zero. So I come back from vacation and I'm noticing too few errors. One is that once you're only. Well, before actually, before going on vacation, I noticed this issue. So a very simple piece of code. And I said, I said take the things you can match. Match them. Take the things you can't match, create them. And then take anything left over that's still on the table and just mark this delete. Yeah, I like that. It worked. Yeah, it works. Take whatever's left over. And delete it. Now what? An abundant die wrote accidentally. Is that in that code? And then I went on a plane and I went, ha, okay. So what ended up happening is. Just a one line bug. When I was comparing. These two things. Let's say. Just from a for loop bug. I'd like double for loop. This will print out twice. So I would say, take this. Swap it out. Take this, swap it out. Take the leftovers and delete them. But these two are the exact same copy of these two. By memory. And so we just mark both of them as deleted. And then journal entry lines lose all their lines. That'll get you. Yes. It was a for loop. Double for the better. Finally it did. Well, I think I should have the time. I'm saying, when I was finding the issue, I was working with it, and then I looked at it like, that doesn't make sense. And I moved on. And then, like, three hours later, I came back and I was like, oh, wait. The whole time I just wasted. It was like, literally, it was like, oh, you're duplicating. I didn't read it. I looked at the coaching like, you're not even in the right area. And I moved on. So that was. The last Tuesday, Wednesday. So I go, okay. Now. So the way you fix this is. What I end up doing here. Something. I found in rubber anymore. It was like, I think I took all those journal entries and I just marked them as not deleted. Or something, but I had to make sure that the issue with writing those kind of SQL queries is that you have to also make sure you're not taking stuff that is actually deleted then marketing noise undeleted, because that would also be a problem. So I took a very long SQL program to specifically find this case and then mark those as undeleted, which is like based on a timestamp and based on some other conditions and based on whether or not other lines have been swapped. Because then I know it's part of the swapping logic. You know, it's not easy to detect them, but once. That's the issue with this. Like, once you've made this issue and you've affected 100,000 journal entries, if you try to find the things with an issue, it's actually kind of a complicated SQL query. To write one that says like this. Everything in this case would only happen if these conditions were true. And it's a nightmare. So that's why all of Lilly. An entire day of last Wednesday goes around. Entire day of last one entire last Wednesday, maybe Thursday, Friday migrations. That must be it. And we're done. Any questions so far? Okay? Bugs happen. Duplicate A for loop. Of the stage. I don't want to replace anything. Because I'm so frustrated. For another reason. Similar to this, but just another dentist. I want to talk about something else. Okay, so here's what happens. So another thing that happens and I can't remember. Yeah, okay. Another thing that happens. Some of us that I log in. Maybe it was Monday. I think. Oh, last week is done. I cleaned this up. There's a sage counterparty bug kind of similar to this migration. Oh, life is good. Whatever. Yeah. Someone says some innocuous dog. He was like. I think. It's like some lines weren't netting out. Like a journal entry sum was an adding up to zero. Granola. And then I find, like, I won't say, about a million journal entries that are off. Or maybe two. I can't remember. Two million. Two million. Or $1 million. So I'm like, go back into this mess. This is Monday. I think what just happened. Why am I. That's why. If you see me stressed out there in a car, that's what I've been up to. So what happened? This makes me realize. Two years ago. Oh, boy. A year and a half ago. A year and a half ago. We rid the visa code now. Turns out it was running to this day. And the basic said something like this. Let's say you mark some vendor object on processing. Right for whatever reason. Let's just set it back to processable again and try again. Maybe something got fixed. Maybe it's processable. Now. And so what happened is on Saturday morning. That code took all the merge unprocessables. That this ETL been setting down processable and set them all back to processable. Again. Is that a modal? What? Where does it run? It's on model. I found it. And so it took, like, a million journal entries, and it said, these are all processable. Now. And because. In the emergency the merge etl. I turned off processing journal entries, any new ones, but it automatically picked up everything that was processable. Again. And it started creating journal entries. Duplicates. Thousands and thousands of duplicates. Where does it do that? You turn off? I thought you turned it off. I turned off the pipeline, but one of the things that that code says is either take any vendor object that's processable. And doesn't have a journal entry, try to do it again. Like the code. The code actually works really well in this case. But it says it doesn't. Look at what data you just downloaded. It always just goes database and say, take every unprocessed vendor object and process it. 's a generic thing, so it's not a part of the accounting detail. It's part of the accounting etf. Oh, sorry. Not part of the. But it doesn't look at what we downloaded. It's just doing that independent of what you download. It'll process new stuff, but also process all the stuff if there's a bug or something. Is that in, like, the JE code specifically, or is it general logic? It's just a conversion. Oh, does it call that crazy big function that's like, process, vendor, object, whatever. So it took all these merge journal entries, which were, for the first time ever, sent us unprocessable. Well, actually, two bugs happened. One. Why do they get on 4? They were set as unprofitable. Because now the ETL, the new ETLs override. It takes priority over the merged general entries. So if there's two duplicates, it would take the merged one and say that's unprocessable. But now they're all processable. And so we generate deposits of digital. Whatever. And then the other thing that happened. Kind of related to that. Okay? Anyways. Then another thing happened, which is actually the week before the Saturday. I think this might run once a week. This whole fail unprocessable thing. What it did is it took all the. Remember that for the last two years we've been collecting manual journal entries from the CSV. And we're sending them to unprocessable because we said, hey, the merge duplicate exists. What happened to Saturday before is that this code took all of these and sent them back to unprocessable false. Or set their processable again. And apparently when I look at the versions of these objects the last one year they kept on processable false true. False, true. False, true, false, true. Like every week it was trying again and then failing again. You know, and on this week, now that we switch the logical order, we went, oh, I'm free. I can convert myself. Started making journal entry line on it from that, which led to a lot of duplicates. The other thing that would happen. This is just another sets of problems that would happen. I don't know. Just go into a bunch of problems. Is that sometimes. One of these would match and convert properly. Another one wouldn't. It would just kind of fail to convert. Let's just say for whatever reason, I couldn't even convert this. Like, if I can't find the account for this, I would just mark this as unprocessable. And then also took the leftover lines. And delete them as well. And so you would have journal entries with only one line. It would swap this one. We couldn't process this one. And then. And then we said take the leftover merge lines and delete them anyways. So you would have general entries with only one line. So these are some of the cases, things that happened. In the last week. The key lesson. I mean, the key, like one of my key takeaways. Is also just like, I don't know what the. I mean, a lot of the data integrity stuff is a function of this. Like do the journal entries add up to the child balances? Do the journal entries or whatever. Like there's some checks we don't have. Building those checks will make us more stable. I don't have a way to check. We still have a way to check if we're download the data properly. So building that is. This is all the screen will lead to some good outcome. Which is we'll have better integrity checks. Which I'm sure you're working on. Also just a reminder that, like, you know, with this kind of stuff that like a bug if it overhyping detail. So if you create a bug that either creates a duplicates or deletes stuff, it affects millions. Of rows. And then. And most of the things that effects are in the past. So no one notices it immediately. It's not like people come to bases and they're looking at the general ledger every day. People are coming to basis and they're running tasks. And so the tasks go from, like, 10,000 to 9,000. Guess what? No one notices. This is why they're silent. Buttons. And then they almost sort of notice stuff. When, like, we try to push. What was this example? Oh, yeah. It was like you try to push a prediction. The prediction creates a manual journal entry in merge. Because using version to push. And then we would have a QuickBooks thing, and then there would be a duplicate, and then all the lines we could do again. There's some, like, weird thing would happen. And Elijah goes, oh, this is one weird prediction. And that seems like not a problem. And it's not even happening that much I believe. This book have a weird setting. Last thing I'll say is this is another issue, that this is a whole other set of issues that are happening. Which is when we used to download the CSV. I only took a few columns from it and saved it down, which is a mistake. I shouldn't have done that. But I did that two years ago. Now when I'm rewrite the code from scratch. I said, well, why don't we just take everything from CFC and save it down? But those two dictionaries have different structures. And so when sometimes. What happens now is that an old CSV object will try to get converted, but it's using new logic and they would fail. To convert. Schemas wouldn't this gamers wouldn't match. Yeah, but the way the code worked is that the schemas would match and then with silent fail, silent, silent. That's fun. And so that's one of the things that I ended up fixing. But because. Everything's running. Everything's running. But then we have all these missing journalists, duplicates only, all sorts of stuff, because that was a conversion failure ratio. That would happen from Noel Schema Newski. You know, the new scheme was a lot better. I just showed, actually, how it's like, this big. It has all this metadata and stuff. Back in the day when I was creating my Norgate, I was like, oh, just take these four fields. And that's all that matters. And now it's like, let's just save the whole thing down. And then, you know. That was another issue that happened. Fix that and then now. And as a function of that, the other schema issue is that. The old object doesn't have any IDs in it. The old CSV object, the new one does. Like counterparty id. Something like referring to like the counterparty you're using for this geometry. But the new code required ids. And so what would happen is nothing's gonna happen. Our cron job would run once a week. It would send all of these things back to processable. They would process, but because the code is looking for a counterparty ID. And there was no counterparty ID field and it would silent fail into a null. All the journalists who learned items created from this process all are missing counterparties, which is a book that Elijah showed today. Sound film. The worst gonna fail. And so now the script I'm running right now. Which hopefully is done by now. Is I fix the code. To now process old objects and new objects the same way. I think I've covered on the basis. So there's like a legacy code and not a legacy code, so that if the. If the vendor object you're getting has an older schema, it can detect that and work with it. Correctly. Testing a little bit, and I merge that code in. So now we have to go fix all the journal entries in the system. Now, usually sometimes I'll go to the database and manually do stuff. I will never let anyone here do that other than me right now. And even that, I'm, like, super scared of. But the consequences. The other thing you can just do. Is you can take every vendor object that has a counterparty in its data, but not in the journal entry line generator. And then set its updated at time to now. The base object. It just got updated. And the ETL takes all newly updated objects and reprocesses them. I didn't know. That it needs to be updated, but just get in that field of the basis. Oh, yeah, yeah. It takes the last runtime and it compares, Captain, that time. Yeah. So instead of going and doing a bunch of database updates to fix this counterparty ID issue, I fix the code merge that in and then I just take everything with the component ID missing and I just set it all to update it up to now so that the next ETL run picks them up. And it will reprocess them. It'll take much longer, but at least it'll. And that's about a million geologies. 195,000. Just a few. So that's why even. That's why. I don't know. I mean, I just want to share a little bit about. The reason. I just want to share this. Or have this access. It's also because of the nature of bugs in our system. Right. Elijah today he goes, oh, this journal entry is missing a counterpart. He sees one example of it, he goes, that's weird. And I go, that is weird. He's not coming and saying a million journal entry line items are missing counterparties because he doesn't know that. And it's hard to know. And so oftentimes, in that example, It took me a while to hunt it, but I can go. Oh, I can fix this issue. But often they're all symptoms. Of deeper, much, much deeper relations. Hips. And if you see a bug, especially a data platform bug in our system, That means that it has spread. Already. It is bred in some unknown way. Maybe that's my takeaway from this whole exercise. For people who are on call. Data platform issues are viruses. That are spread out. Some of them I know. Some of them I don't know. Like, at some point I just go, okay, this doesn't matter anymore. Like, right now. 235,000 journal entries that do not add up to zero. No, no, no. Do not add up to zero. They have to add up to zero. So they're breaking rule. Okay, but most of them are old. Are. All of them are old. I took all the new ones and fixed the bug. And then the question is, should I add somebody? Have to go update all the old ones. And we process them. Which I wrote a script for an hour in the morning, but I need this counterfeiting thing to work first. So I'm going to run that in the morning. I was going to say, yeah. Things. But at some point, I was looking at this issue today. Like 400 of them are missing. Can't find the account that they were from. I know why it's happening. And I go. But not a fix this. I don't know how to catch that. But if it's above 1000 or 2000. Did you see an issue of positive. And also, I'll just say that so many times during this whole exercise, there were these moments that were like, I am done fixing this bucket. It's clear to me. And fix it. And then I go, you know what this is? Data platform. And data platform is a nightmare. And I've never really fixed it. And that's just an instinct that I have. Partly just being here for about two years. There's like a Spidey sense where it's like even that. I remember this whole thing started because Elijah said there was five General Lynches missing, and I did some SQL query and I thought it was a thousand. And at some point I saw number 150,000 and what that can be, right? And I went back to the old SQL program. It's 1000. But it was 135,000 thing and so I was trying to think about how to teach that sense. There's no way to teach it other than time and energy, but I would just say that data platform errors simply put, journal entries Mark just deleted that isn't deleted. Counterparty is missing when there's a counterpart. General Adam showing someone to the trial balances. That they're supposed to. Influx or something. Has gone very wrong. And if it's gone, run for one book. It's gone wrong for every book of that same integration. Type. So if it's gone wrong for one asset, Are there ways. But if it does fail to fail where you have to have so there is so back in the day. None of these are active anymore, but there are a few ways to check. There are a few. I mean, at the same time, accounting is also a bunch of. There's special wasted charge, right? So QuickBooks, for example, will tell you what your bank account. I'll give you an example. We'll say your bank account is $13,000. So if I summed up all the transactions that I have that are not deleted it, they should not get their $10,000. It doesn't add up. Something's gone wrong. That's a very simple. So there's variations of that. So Joe is now in charge of our project. We're talking about after this, right? I know this is all good timing. Okay. And so called like data integrity or something and it'll have a few checks like that. Essentially that we never build because we never have the people to do it. And they've talked to you. That would be data checking is the outcome. Accounting software is, like, you probably spend, like, 20, 30% time building all kinds of checks. Every step. We have to check. It is actually a surprise without a wave. This far. That's the part I got really frustrated last week. Now because of anything else? Just, like, I was looking at the size of this bug, and I was like, how are we not getting, like, 100,000 customer service calls right now? You know what I'm saying? But that's what I'm saying. People aren't looking at our ledger directly, and there's too much data. Right. So they can't. They're not even noticing. They're seeing the outputs of the data. So the matching algorithm is wrong, by the way, matrix is from. Well, some of the journal entry lines are missing. Right. And so you can't match if one side of it's missing. So a bunch of transactions can find their relevant journal entry items, and so they get predicted on. So customers go, hey, these transactions are getting predicted on. They're not matched, and that we have predictions on them. Why is that happening? And then Elijah goes, I don't know. Let me go check in. So he goes through this whole processor trial. Check, I think. Arrows. You were looking at it. And I think Elijah ends up doing this whole investigation to be like, oh, that's so weird. This bank transaction does match to this journal entry, they send me a screenshot, and that journal entry is marked as deleted in our system. So weird. And then that starts the hunt. Probably not hing even today when he's like the counterparty ID is not. I went, oh, no. Something has gone wrong that was all about old objects. So anything that was converted as an old object. Then God and I disabled the one week unprocessable thing that we don't need that at all. So I have a question. Counterparty being missing. Is that just like it's missing at all? Because I thought it could be missing. How to buy the object isn't missing. They have pretty ID field of the journalism. Is that weird? It is weird. Why? I thought it didn't have to be. It didn't have to be, though. This is also why it's a silent fail. How did you know that? It should have been there. The vendor object has a counterparty field. Which has a value. So I literally just random query called does the vendor object have a field? Does the actual object out have an id? Something has gone awry. Set all their updated ads to now retry the etl. No more issues. It's all fixed. Right. So we'll see. In the morning, we'll wake up. And I'm gonna run this exact same query that just says, does the vendor object have accountability? Now, there are examples of this now working. So because part of that, a lot of these, the other thing I'm learning right now, Is that this whole unprocessable thing kind of really screwed us. I mean, not really. What's happening? Is that okay? Remember that we're getting CSVs 90 days. Like we will only update the last 90 days of journal entries and that CSV report. When we swap from merge to QuickBooks, these old journal entries got activated. And the merged ones got deactivated. That was through a bug. It set back to unprocessable, and then it swaps them. The order of operations was that. The cron job runs sets the all the old CSV report objects to processable they run through this new code. They kind of silent fail sometimes. Most of the time they actually succeed in swapping themselves out for the merge object. But that data is old. It hasn't been touched in a while. And so. And that data refers to. Accounts and counterparties that are now deleted. And so those accounts of counterparties have a parenthesis deleted around them. In their official database. Name. But the old object still has a non deleted version of it. And so when you try to look for it, you can't find it in that affair or something already sensitive to knowledge. We also have a visa code, which I still need. The recent counterparties silent fail is because we said, look, even if there's an counterparty bug, At least we have the German entry. We'd rather have the journal entry. Than break the ETL for a counterparty program. I don't know about that. That's a trade. So one of the things that other things I'm considering, which I don't have time for, is you take all the old report objects and you requer all of them. In, like, one ETL for, like, 15 hours, and you just swap them all out with the new one s. Then that way you don't have to worry about any of the stuff breaking because now they're all the new object. There's no more. Because I think all the bugs that are happening right now are like old object related. They haven't updated. The other thing I've been doing is I've been taking a script. This is my local script. My local script says take all the ones that are breaking something. And just refetch those and get the new CSV report objects for those. And. Just re reupload those. But the ones that are like netting out to zero or balanced out or whatever. Like because I would have to otherwise go and redownload a few million QBO general reports which I just don't know how long they would take. This in this case like right now 250,000. I'm seeing errors with. I'm like, I can do 200,000. Like, it'll take, like, three, four hours. And it'll run in the background of my computer tomorrow morning. And then we'll go on. The current journal entries are some combination of merged journalism which still exists. Not all of them, so got swapped over. Old qbo CSV report journal entries and then new CSV report journal entries. Luckily, there's no instance of the old ones and the new ones. Clash. There's no duplicates between them because we use the same Form ID system. So they override each other. So that you know, but I guess. So if you look at a journal entry, you look at their vendor object, you'll see one of those three variations. And any of the bugs right now is because a lot of the bugs are because one of the lines is a CSV and the other one's a merged one or something. Which I'm going and fixing. Like today, for example. I'll let you get 15,000 errors. And I ran the script and now it has like 300. I looked at the 300. And I was like, Okay? Maybe, I don't know. They're pointing to delete an accounts and stuff. So I have to. Now one of the, like, one of the benefits, for example, is if a journal in China then points to a delete an account. No one looks at deleted accounts. So maybe it doesn't matter. It doesn't matter, but at the time, at some point, you're just playing a sizing game of, like, how much you're gonna go backfill and fix all those data. They're doing stuff. I just wanted to pick the largest error fields. And work from them. First. Yes. Two questions. Yeah. Question one. Why not? So we chose to do, like, an update on demand type of thingy. You had mentioned this going back, fixing all of them. Yes. Technically. Should we have done a full thought about it? I'm looking back. I don't know. That's a good question. So should we have just downloading all of the data from all of history again? Right from scratch. Swapped everything over and then delete all the merged stuff or do whatever you want with them. And just get off of it. Fully. There are a few things that happen, actually, that made this difficult, actually. I remember why I thought this. We still push through merge. And so there was some merged journal entry object that we're getting as an intermediary object that then the ETL would then overwrite later. But you would need that intermediary object for whatever reason, because the ETL hadn't run for well, but you would have this object, and that being otherwise, your sister would be wrong. If you were in tasks or something. Because that makes your. I also just thought that if you've seen our onboarding. Downloading all the data. Took about a few days. Or like a day? Let's just say per book. And we have a few hundred books. I looked at that and I went, you know what? Live migrating systems. Is what I do that again? I don't know. Again. Bigger scale and bigger team. We would have done it in batches. I just don't know. It should be a conversation. I'll be probably for the next migration. If I need, I would build migration. Should we've done a bachelor's or not. I personally kind of do like this kind of look at the end of the day. This code was written at the beginning of September. It's middle. Application that has 16. That's when I found some of the initial bugs. So let's say it was times September 1st. We're two weeks in. September 16th, right? Like two, two and a half weeks into using. Just let's say three weeks using the system. I accept the number of bugs. And the number of people who found it and the number of effects it had on customer. Much, much lower. Like to the customer still. It's like, oh, my counterparty. I think this is what I took from Malaysia. They are bad bugs, but. They're not showing up in this way that's, like, super destructive. Just luck with big migrations. It's like if it breaks, if it starts from scratch again, you have to do all these other things. With live migrations, you don't have that issue. And you can fix things, you can go kind of thing. Because the old data, I mean, the only thing that really screwed this. More than anything. Is that whole Cron job running in the background, setting all those unprocessables to. I mean, if I know the bad existed. That would be differently. That was my next question. But actually one other. If that didn't exist, we wouldn't have found this out for a long time. But also it would have been a slower decay because we would have seen one counterparty issue and fix that. This, scaled it up because it took all these things and a million journal entries and then flipped them over. And so I just didn't know this thing was flying around in the background. I didn't think about it during. As soon as I was like, why is these things coming up? It was all at the same time. It was like, not our etl. Time of Abel is the HL network. It was like, Saturday morning. All of them were like, on Saturday morning. And another, like, million journal entries would try again to like. I was like, what is happening? Right now. Also just a friendly reminder on why versions is an amazing table. It would be nice. Here where there is some. There is some bug, like a few months ago. Where there was some random shit running in modal. That was fucking up Wreck. I don't know if you remember that. Very similar. What is happening? Who's doing this? Why does unprofitably existing. Why not? Just. We actually have data errors, stuff like. It's like I have a germ entry. Someone's saying there is a general entry. I can't convert it. Something's gone wrong. I need to know what's there so I can figure out how to convert. When you say you mean you manually look at it? Yeah. Someone has a manual. Yeah, I tuned me like when I hear this. Right. There's two kind of big problems. Why is that on day one? And something's on process. Right. That's huge. Because usually you want to. Like you said, you don't want to choose information. That's something definitely to clean up, but the second part is, like, well, along with that. There's this unique situation with cvs. That updates the leads. How you track it. Right. I think that something can do about it. I think, let's say there need to be some kind of systematic way to check. It's so bad. So that no matter what happens, you can go. I've done something wrong. Everything's good, right? So, for example, you can't say that. You have something missing. Or one side is off. You can always check to say he is in the CVS. He is the new version. He has a total cabin. It's total credits I get from Geo account level and then convert that and then to say does that too tie to what I update to and number of records at the moment? Black can also basically do all the checks. At a time we digest all the information. Right. That's one thing, I think. Second piece that's harder to do is this migration. And then one you have changing the past. That's tough. It's just. Tough. Yeah, I think usually we do is that you do it parallel. You do it parallel. But our system does not allow for balance because then you have to look at data. We can mark them as the leader. That or something. We want to set free laundry on stuff. Pre vomit. And shower. Yeah, but at some point in the live system, we have to do something, right? Because there's also real data coming in at the same time. I would say this is the kind of project that would have, like, two months. And then regular. Head. So for us. It's like, you know, that's also why I wanted to do it, just because it's so complicated. And mention Mac and blend. We're just breaking. We're just breaking, okay? Trying to get it done. But it is a lot of just moving pieces and stuff. So we'll see after this counterparty ID thing. If everything's okay. I wouldn't be surprised if there was some duplicate. I was doing some data surgery in there. I was doing all kinds of stuff in the code base. And so in the process of trying to fix stuff, I wouldn't be surprised how this stuff is broken in the thing. But I do think integrity, textile level. Will help us a lot. There is, like, one issue happening, right? I saw this last week. There was a sage bug, and right now. I need to make a table. Otherwise. Don't forget this. But it's okay. I don't also take up the tables. You try it. But 70 sage counterparties. Have switched to another counterpart. And so if you went to an auto shop, And you journal entry said, like, you know, spend $15 in auto shop. It now says spend $15 on auto mechanics. I've had a daycare. I just, like, swapped the counterparty. It's only for 70 counterparties. I know why it happened. I know the whole thing as to why it happened, and I looked at it. And I fixed. There was like 900 of them. I got it down to 70, and the 70. Were like. And then other systems are now built on top of that assumption. And so I need to go. This is another data surgery you have to go do. But there's only 70 stage counterparties that are doing this. You'll see it the way you know. Is that the. You'll know because the description of the counterparty switches alongside the vendor object. When you're doing the vendor object swap, the description should stay in sync, as both data sources are saying the same description for the counterparty. But the description swapped as well. So that's how you know that the swapping fit or something went all right. This actually is the merge thing. Merge did a very annoying thing. For vendors? No. Sage has a vendor id, has something called record number, which is the unique ID for Sage. Merge doesn't use record number. I messaged him. I said, why don't you do that? They said it was easier for us to not use it. I'm like, not use the primary key. Of the fucking thing was easier for them. And I understand why they're saying that we talk about in a second. And so they instead took two different keys, one called Vendor id, one called Custody, and then made that the remote ID field, even though remote ID specifically is what is the primary key of the underlying system, and they just didn't use record number but when we were building our system, we wanted to build a record number, because that is the official primary cube thing. What would happen is that the vendor ID and the primary key would sometimes be like the number 1348. Or something. And the sage does a weird thing. Sage has their vendor keys. Start with V and they go 0 0. One. That's great. It's a vendor key. Okay. Their record numbers look like this. You're like, okay, these are very distinct things. So I wrote some code with that assumption. It turns out in some books. The record number looks like this. Okay? And some books. The record number looks like this. Outside the vendor. Vendor id. So you're spotting. So I was swapping these. The vendor ID with the record number. It only happened very rarely because they were both large numbers. But it would happen. And so it swapped the wrong thing in. It's only 70 now. 70? Count. So I have a script on my computer right now to self, a Mac, and then I wrote that last Friday. Didn't get a chance to run it. And then on Monday when I came in, Other stuff was. I think, fundamentally, I think this is significant. Like a product or a data. Before we go, something we have to say. How do I know? That I've done right. But you almost have to do that first. That's always the case. And you know otherwise. The wrong data is just. The worst thing, basically. Well, I think in our integrity check to shine counterparty ID checks like summation by counterparty, 80 by account. I had it, but I don't know how to fix. This one's super weird. And it'll be off by a little bit in some places. But I do think we have some Lilian auto shop and a daycare center swapped. Somewhere in the name of. Was this informative? Did people learn how to suffer? Tm should we do more of these and things break? The next course of action is the Integrity Project. That's the biggest. Outcome out of this to make sure that we have data integrity when we do stuff. And I hope that the etl works. Out of this. Come. I hope it works. Oh, the only thing. I'm going to run one book right now. Before I leave so that I can actually check that that book fixes itself. At least. And if that doesn't run, then I'll be here till midnight. Trying to take time. That's what I'm going to do right after this. Just make sure we. Cool. That details how much? I highly recommend, if people are interested, to look at the ingestion folder. Under services, under journal entries. Under quickbooks. Ingestion services, chairman cheesecloth books. You will see all this coded. All the report code swapping stuff. Yeah. All this stuff about. And you'll see a lot of that. I tried to document a lot of it. As I can. But also. One of my jobs is actually to transfer all this knowledge. To someone technical. If anyone has volunteers. Fine. I like. They'll give it to some people. You seem interested. Looks away. I don't know if I want to do this all day, every day. It's not all the area. I'll say this about data engineering, which is that I haven't thought about this. Stuff for the last six months. We would have issues here and there with Merge, but we have a pretty stable data platform. And then we head off merging now, it's like change everything. This, like, actually kind of painful and hard to do. But it is like literally all of our data is getting moved over which is like actually a nightmare to do. But I think I'm doing good Some ideas in job of it for the number of bucks we're getting. But we have Sage and Z I sub been able to start saving zero because then hunting down these counterfeiting party, all these other bugs. So that's the only other issue that I'm personally dealing with. But one stage and zero are over, then this is actually. For now. The only other thing, migrate over actually accounting events, which we don't build that much stuff on top of, so you have to get a merch at time. I hold the core products of building job which. We can still pull those from Merge or something. We have to swap overall. The pushing logic team is not virgin use. As bones. So that's the next project after this. So that all the posting journal entries and posting all this stuff is done. For that work. What's the sort of microphone? Completely. Get out of the roads because. Yeah. So once the entire system uses fully pass through. Then switching off a merch is just an authentication. And so which ones to do, they just re sign the contract of Marge and so that way we have it for at least one more year. And in that one year, we're going to talk. I mean, they'll know. When we ask it. But we're going to ask for all the authentication. I'll do authentication. Our systems as well. And we'll let the contract. How much does merge? Where is that secret? Application. I think it's like. $20 a book per month. Or connection for money. And so if you do 1,000 connections, That's one brand. When I first heard, I was like, this is too cheap. But actually, when you say 100 grand a year, $20 a month. For connection per month. So we probably have some discontent. Or can I go look at this kind of party thing? 